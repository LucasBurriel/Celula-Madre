# DGM Original vs Nuestro DGM ‚Äî Comparaci√≥n Detallada

## Arquitectura General

| Aspecto | DGM Original (Sakana) | Nuestro DGM |
|---------|----------------------|-------------|
| **Qu√© evoluciona** | C√≥digo completo del agente (coding_agent.py + tools/ + prompts/) via git patches | Solo el system prompt del agente |
| **C√≥mo muta** | Coding agent modifica su propio c√≥digo fuente dentro de Docker | LLM genera un nuevo system prompt |
| **Evaluaci√≥n** | SWE-bench (GitHub issues reales) o Polyglot benchmark | Tasks de coding custom (regex, json_parser, etc.) |
| **Selecci√≥n** | score_child_prop (sigmoid + 1/(1+children)) | score_child_prop (misma f√≥rmula) ‚úÖ |
| **Archive** | keep_all (todo lo que compila) | keep_all ‚úÖ |
| **Diagn√≥stico** | o1 (fuerte) analiza logs + tests + c√≥digo del agente | Mismo modelo (Gemma 4B) analiza logs + tests |
| **Implementaci√≥n** | Coding agent (Claude) edita su propio c√≥digo con tools (bash, editor) en Docker | LLM genera nuevo prompt directamente |
| **Infraestructura** | Docker containers, git patches, SWE-bench harness | Python directo, tempdir, pytest |

## Diferencias Cr√≠ticas

### 1. üî¥ Lo que evoluciona (MAYOR DIFERENCIA)
**DGM original**: Evoluciona el **c√≥digo completo** del agente ‚Äî puede a√±adir tools nuevos, cambiar el flujo de `forward()`, modificar prompts, agregar utilities. Los cambios se guardan como git patches que se acumulan.

**Nuestro DGM**: Solo evoluciona el **system prompt**. El c√≥digo del agente (`CodingAgent.forward()`) es fijo ‚Äî siempre hace: generar c√≥digo ‚Üí escribir ‚Üí pytest ‚Üí (optional) fix.

**Impacto**: El DGM original puede descubrir mejoras arquitect√≥nicas (nuevos tools, retry strategies, etc.), no solo mejoras de instrucciones. Nuestro DGM est√° limitado a prompt engineering evolutivo.

### 2. üî¥ Modelo de diagn√≥stico vs agente
**DGM original**: Usa **o1** (reasoning model fuerte) para diagn√≥stico, y **Claude** para implementaci√≥n. Son modelos diferentes y ambos potentes.

**Nuestro DGM**: Usa **Gemma 4B** para ambos. Un modelo d√©bil diagnosticando sus propios errores y proponi√©ndose mejoras a s√≠ mismo.

**Impacto**: La calidad del diagn√≥stico es mucho peor. El modelo no tiene la capacidad de razonamiento profundo para identificar por qu√© falla.

### 3. üü° Evaluaci√≥n
**DGM original**: SWE-bench Verified ‚Äî 500+ issues reales de GitHub. Evaluaci√≥n robusta y reproducible.

**Nuestro DGM**: 3-4 tasks custom. Poca diversidad, alta varianza.

### 4. üü° Docker isolation
**DGM original**: Cada mutaci√≥n corre en un Docker container aislado. El agente modifica c√≥digo, se commitea, se eval√∫a. Si rompe algo, el container se destruye.

**Nuestro DGM**: Todo corre en el mismo proceso Python. Sin aislamiento.

### 5. üü¢ Selecci√≥n (CORRECTA)
Ambos usan `score_child_prop`:
```
probability = sigmoid(10*(score-0.5)) √ó 1/(1+children_count)
```
Favorece agentes con buen score que no han sido explorados mucho. ‚úÖ

### 6. üü¢ Archive (CORRECTA)  
Ambos usan `keep_all` ‚Äî todo agente que compila/corre se mantiene en el archivo. ‚úÖ

### 7. üü° Tipos de diagn√≥stico
**DGM original**: 4 tipos de prompts de diagn√≥stico:
- `solve_empty_patches` (25% chance) ‚Äî cuando el agente no genera patch
- `solve_stochasticity` (25% chance) ‚Äî manejar varianza del agente
- `solve_contextlength` (conditional) ‚Äî cuando excede contexto
- Normal ‚Äî analizar un issue espec√≠fico que fall√≥

**Nuestro DGM**: Un solo tipo de diagn√≥stico (analizar tarea fallida).

### 8. üü° Gating
**DGM original**: `filter_compiled` ‚Äî solo agentes que compilan y no tienen todos patches vac√≠os entran al archive. Opcionalmente `keep_better` con noise leeway.

**Nuestro DGM**: Todo entra al archive sin filtro. ‚úÖ (coincide con default `keep_all`)

## Qu√© necesitamos cambiar para ser fieles al DGM

### Prioridad Alta (afecta validez del paper)
1. **Evolucionar c√≥digo, no solo prompts** ‚Äî El claim central de DGM es self-improvement del c√≥digo. Si solo evolucionamos prompts, no estamos replicando DGM, estamos haciendo "evolutionary prompt engineering".
2. **Usar modelo fuerte para diagn√≥stico** ‚Äî Necesitamos al menos un modelo razonable para diagnosticar. Opciones:
   - Qwen3-30B (local, m√°s capaz) para diagn√≥stico + Gemma para ejecuci√≥n
   - Claude/GPT via API para diagn√≥stico (costo)

### Prioridad Media
3. **Docker isolation** ‚Äî Para evolucionar c√≥digo de forma segura necesitamos containers
4. **M√°s tasks** ‚Äî 3-4 no es suficiente. Necesitamos 20+ tareas diversas
5. **M√∫ltiples tipos de diagn√≥stico** ‚Äî Al menos empty_patches y stochasticity

### Prioridad Baja
6. **Git patches** ‚Äî Nice to have para tracking, no esencial
7. **Two-tier evaluation** ‚Äî DGM eval√∫a en subset chico primero, si pasa threshold eval√∫a en set grande
8. **Post-improvement diagnosis** ‚Äî Diagnosticar si la mejora realmente ayud√≥

## Opciones Realistas

### Opci√≥n A: Prompt Evolution (lo que tenemos)
- ‚úÖ Funciona, es r√°pido, corre local
- ‚ùå No es DGM, es "evolutionary prompt engineering"
- Sirve como **baseline** para comparar con DGM real

### Opci√≥n B: Code Evolution con Gemma
- Gemma modifica su propio c√≥digo (el CodingAgent)
- Problema: Gemma 4B no puede generar c√≥digo complejo de forma confiable
- Necesitamos Docker para aislar mutaciones

### Opci√≥n C: Code Evolution con Qwen (diagn√≥stico) + Gemma (ejecuci√≥n)
- Qwen3-30B diagnostica y genera patches de c√≥digo
- Gemma 4B es el agente que se eval√∫a en tasks
- M√°s fiel al DGM (modelo fuerte para diagn√≥stico, modelo evaluado diferente)
- Requiere cargar ambos modelos (problema de VRAM que ya tuvimos)

### Opci√≥n D: API para diagn√≥stico + Gemma local para ejecuci√≥n
- Claude/GPT para diagn√≥stico (pocas llamadas, bajo costo)
- Gemma local para ejecuci√≥n (muchas llamadas, gratis)
- M√°s fiel al DGM original (que usa o1 + Claude)
- Costo estimado: ~$1-5 por run completo
